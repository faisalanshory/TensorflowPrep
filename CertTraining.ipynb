{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Given two arrays, train a neural network model to match the X to the Y.\n",
    "# Predict the model with new values of X [-2.0, 10.0]\n",
    "#\n",
    "# The test infrastructure expects a trained model that accepts\n",
    "# an input shape of [1].\n",
    "# Do not use lambda layers in model.\n",
    "#\n",
    "# Desired loss (MSE) < 1e-4\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def solution_A1():\n",
    "    X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0,\n",
    "                 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
    "    Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,\n",
    "                 12.0, 13.0, 14.0, ], dtype=float)\n",
    "\n",
    "    #Create a model network\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "\n",
    "    #Compile and fit the model\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    model.fit(X, Y, epochs=10000)\n",
    "\n",
    "    print(model.predict([-2.0, 10.0]))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_A1()\n",
    "    model.save(\"model_A1.h5\")\n",
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# Given two arrays, train a neural network model to match the X to the Y.\n",
    "# Predict the model with new values of X [-2.0, 10.0]\n",
    "#\n",
    "# The test infrastructure expects a trained model that accepts\n",
    "# an input shape of [1]\n",
    "# Do not use lambda layers in model.\n",
    "#\n",
    "# Desired loss (MSE) < 1e-3\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def solution_B1():\n",
    "    X = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0], dtype=float)\n",
    "    Y = np.array([5.0, 7.0, 9.0, 11.0, 13.0, 15.0, 17.0], dtype=float)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "\n",
    "    # Compile and fit the model\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.0005),\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    model.fit(X, Y, epochs=8000)\n",
    "\n",
    "    print(model.predict([-2.0, 10.0]))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_B1()\n",
    "    model.save(\"model_B1.h5\")\n",
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# Given two arrays, train a neural network model to match the X to the Y.\n",
    "# Predict the model with new values of X [-2.0, 10.0]\n",
    "# The test infrastructure expects a trained model that accepts\n",
    "# an input shape of [1]\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# Desired loss (MSE) < 1e-4\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def solution_C1():\n",
    "    X = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
    "    Y = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=float)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.0005), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    model.fit(X, Y, epochs=4500)\n",
    "\n",
    "    print(model.predict([-2.0, 10.0]))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_C1()\n",
    "    model.save(\"model_C1.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification Without Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "#\n",
    "# Build a Neural Network Model for Horse or Human Dataset.\n",
    "# The test will expect it to classify binary classes.\n",
    "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
    "# Don't use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 83%\n",
    "# ======================================================================================\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "def solution_A2():\n",
    "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
    "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
    "    local_file = 'horse-or-human.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/horse-or-human')\n",
    "\n",
    "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
    "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
    "    local_file = 'validation-horse-or-human.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/validation-horse-or-human')\n",
    "    zip_ref.close()\n",
    "\n",
    "    #Training Dataset\n",
    "    TRAINING_DIR = './data/horse-or-human'\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale = 1/255,\n",
    "        fill_mode ='nearest',\n",
    "        rotation_range = 30,\n",
    "        horizontal_flip = True)\n",
    "\n",
    "    train_generator= train_datagen.flow_from_directory(\n",
    "          directory = TRAINING_DIR,\n",
    "          batch_size = 10,\n",
    "          class_mode = 'binary',\n",
    "          target_size = (150, 150)\n",
    "      )\n",
    "\n",
    "    #Validation Dataset\n",
    "    VALIDATION_DIR = './data/validation-horse-or-human'\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale = 1/255)\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "          directory = VALIDATION_DIR,\n",
    "          batch_size = 10,\n",
    "          class_mode = 'binary',\n",
    "          target_size = (150, 150)\n",
    "      )\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(generator=train_generator,\n",
    "                        validation_data=val_generator,\n",
    "                        epochs=30)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=solution_A2()\n",
    "    model.save(\"model_A2.h5\")\n",
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# Build a classifier for the Fashion MNIST dataset.\n",
    "# The test will expect it to classify 10 classes.\n",
    "# The input shape should be 28x28 monochrome. Do not resize the data.\n",
    "# Your input layer should accept (28, 28) as the input shape.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "#\n",
    "# Desired accuracy AND validation_accuracy > 83%\n",
    "# =============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def solution_B2():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    training_images = training_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # End with 10 Neuron Dense, activated by softmax\n",
    "        tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(training_images, training_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_B2()\n",
    "    model.save(\"model_B2.h5\")\n",
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# Create a classifier for the MNIST Handwritten digit dataset.\n",
    "# The test will expect it to classify 10 classes.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "#\n",
    "# Desired accuracy AND validation_accuracy > 91%\n",
    "# =============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def solution_C2():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "    training_images = training_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    training_images = training_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        # End with 10 Neuron Dense, activated by softmax\n",
    "        tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # TRAIN\n",
    "    model.fit(training_images, training_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_C2()\n",
    "    model.save(\"model_C2.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================\n",
    "# Build a classifier for the Human or Horse Dataset with Transfer Learning.\n",
    "# The test will expect it to classify binary classes.\n",
    "# Note that all the layers in the pre-trained model are non-trainable.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The horse-or-human dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
    "# Inception_v3, pre-trained model used in this problem is developed by Google.\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 97%.\n",
    "# =======================================================================================================\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "def solution_A3():\n",
    "    inceptionv3 = 'https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    urllib.request.urlretrieve(\n",
    "        inceptionv3, 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "    pre_trained_model =  InceptionV3(input_shape=(150, 150, 3),\n",
    "                                    include_top=False,\n",
    "                                    weights=None)\n",
    "\n",
    "    pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "    for layer in pre_trained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    last_layer =  pre_trained_model.get_layer('mixed7')\n",
    "\n",
    "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
    "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
    "    local_file = 'horse-or-human.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/horse-or-human')\n",
    "    zip_ref.close()\n",
    "\n",
    "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
    "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
    "    local_file = 'validation-horse-or-human.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/validation-horse-or-human')\n",
    "    zip_ref.close()\n",
    "\n",
    "    train_dir = './data/horse-or-human'\n",
    "    validation_dir = './data/validation-horse-or-human'\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale = 1/255,\n",
    "        rotation_range = 30,\n",
    "        horizontal_flip = True,\n",
    "        shear_range = 0.1,\n",
    "        zoom_range = 0.2)\n",
    "\n",
    "    train_generator=  train_datagen.flow_from_directory(\n",
    "          directory = train_dir,\n",
    "          batch_size = 10,\n",
    "          class_mode = 'binary',\n",
    "          target_size = (150, 150)\n",
    "      ) \n",
    "\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1 / 255)\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        directory=validation_dir,\n",
    "        batch_size=10,\n",
    "        class_mode='binary',\n",
    "        target_size=(150, 150)\n",
    "    )\n",
    "\n",
    "\n",
    "    x =  tf.keras.layers.Flatten()(last_layer.output)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if (logs.get('accuracy') > 0.97):\n",
    "                print(\"\\nReached 97% accuracy\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    callbacks = myCallback()\n",
    "\n",
    "    model = Model(pre_trained_model.input, x)\n",
    "\n",
    "    model.compile(optimizer=RMSprop(lr=0.0001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model.fit_generator(train_generator,\n",
    "                        validation_data=val_generator,\n",
    "                        epochs=15,\n",
    "                        verbose=2,\n",
    "                        callbacks=[callbacks])\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=solution_A3()\n",
    "    model.save(\"model_A3.h5\")\n",
    "\n",
    "# ========================================================================================\n",
    "# Build a CNN based classifier for Rock-Paper-Scissors dataset.\n",
    "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
    "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
    "# Don't use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
    "#\n",
    "# Desired accuracy AND validation_accuracy > 83%\n",
    "# ========================================================================================\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def solution_B3():\n",
    "    data_url = 'https://github.com/dicodingacademy/assets/releases/download/release-rps/rps.zip'\n",
    "    urllib.request.urlretrieve(data_url, 'rps.zip')\n",
    "    local_file = 'rps.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/')\n",
    "    zip_ref.close()\n",
    "\n",
    "    TRAINING_DIR = \"data/rps/\"\n",
    "    training_datagen = ImageDataGenerator(\n",
    "        rescale=1 / 255,\n",
    "        validation_split=0.2)\n",
    "\n",
    "    train_generator=training_datagen.flow_from_directory(\n",
    "        directory=TRAINING_DIR,\n",
    "        batch_size=10,\n",
    "        class_mode='categorical',\n",
    "        target_size=(150, 150),\n",
    "        subset='training')\n",
    "\n",
    "    valid_generator = training_datagen.flow_from_directory(\n",
    "        directory=TRAINING_DIR,\n",
    "        batch_size=10,\n",
    "        class_mode='categorical',\n",
    "        target_size=(150, 150),\n",
    "        subset='validation')\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_generator,\n",
    "              validation_data=valid_generator,\n",
    "              epochs=5,\n",
    "              validation_steps=6,\n",
    "              steps_per_epoch=20)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=solution_B3()\n",
    "    model.save(\"model_B3.h5\")\n",
    "\n",
    "# =======================================================================================================\n",
    "#\n",
    "# Build a CNN based classifier for Cats vs Dogs dataset.\n",
    "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
    "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
    "# Don't use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is originally published in https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 72%\n",
    "# ========================================================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def solution_C3():\n",
    "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/cats_and_dogs.zip'\n",
    "    urllib.request.urlretrieve(data_url, 'cats_and_dogs.zip')\n",
    "    local_file = 'cats_and_dogs.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/')\n",
    "    zip_ref.close()\n",
    "\n",
    "    BASE_DIR = 'data/cats_and_dogs_filtered'\n",
    "    train_dir = os.path.join(BASE_DIR, 'train')\n",
    "    validation_dir = os.path.join(BASE_DIR, 'validation')\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1 / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_dir,\n",
    "        batch_size=10,\n",
    "        class_mode='binary',\n",
    "        target_size=(150, 150))\n",
    "\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1 / 255)\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        directory=validation_dir,\n",
    "        batch_size=10,\n",
    "        class_mode='binary',\n",
    "        target_size=(150, 150)\n",
    "    )\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.002),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=15,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=50,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # DO NOT CHANGE THIS CODE\n",
    "    model = solution_C3()\n",
    "    model.save(\"model_C3.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================================================================\n",
    "# Build and train a binary classifier for the IMDB review dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is originally published in http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 83%\n",
    "# ===========================================================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def solution_A4():\n",
    "    imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "    train_data, test_data = imdb['train'], imdb['test']\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "\n",
    "    testing_sentences = []\n",
    "    testing_labels = []\n",
    "\n",
    "    for s, l in train_data:\n",
    "        training_sentences.append(s.numpy().decode('utf8'))\n",
    "        training_labels.append(l.numpy())\n",
    "\n",
    "    for s, l in test_data:\n",
    "        testing_sentences.append(s.numpy().decode('utf8'))\n",
    "        testing_labels.append(l.numpy())\n",
    "\n",
    "    training_labels_f = np.array(training_labels)\n",
    "    testing_labels_f = np.array(testing_labels)\n",
    "\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type = 'post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "\n",
    "    # Fit tokenizer with training data\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Training sequences and padded\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "    # Testing sequences and padded\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(training_padded, training_labels_f, epochs=30, validation_data=(testing_padded, testing_labels_f))\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_A4()\n",
    "    model.save(\"model_A4.h5\")\n",
    "\n",
    "# ===================================================================================================\n",
    "# Build and train a classifier for the BBC-text dataset.\n",
    "# This is a multiclass classification problem.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is originally published in: http://mlg.ucd.ie/datasets/bbc.html.\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 91%\n",
    "# ===================================================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def solution_B4():\n",
    "    bbc = pd.read_csv(\n",
    "        'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/bbc-text.csv')\n",
    "\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type = 'post'\n",
    "    padding_type = 'post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_portion = .8\n",
    "\n",
    "    sentence = bbc['text']\n",
    "    label = bbc['category']\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in sentence:\n",
    "        sentences.append(i)\n",
    "\n",
    "    for j in label:\n",
    "        labels.append(j)\n",
    "\n",
    "    # Using \"shuffle=False\"\n",
    "    training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(sentences,\n",
    "                                                                                                    labels,\n",
    "                                                                                                    train_size=training_portion,\n",
    "                                                                                                    shuffle=False)\n",
    "\n",
    "    # Fit tokenizer with training data\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "    validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "    training_label_seq = np.array(label_tokenizer.texts_to_sequences(training_labels))\n",
    "    validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_padded, training_label_seq, epochs=50, validation_data=(validation_padded, validation_label_seq),\n",
    "              verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_B4()\n",
    "    model.save(\"model_B4.h5\")\n",
    "\n",
    "# =====================================================================================================\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
    "#\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# Dataset used in this problem is built by Rishabh Misra (https://rishabhmisra.github.io/publications).\n",
    "#\n",
    "# Desired accuracy and validation_accuracy > 75%\n",
    "# =======================================================================================================\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def solution_C4():\n",
    "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sarcasm.json'\n",
    "    urllib.request.urlretrieve(data_url, 'sarcasm.json')\n",
    "\n",
    "    with open(\"/content/sarcasm.json\", 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type = 'post'\n",
    "    padding_type = 'post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for item in dataset:\n",
    "        sentences.append(item['headline'])\n",
    "        labels.append(item['is_sarcastic'])\n",
    "\n",
    "    training_sentences = sentences[:training_size]\n",
    "    validation_sentences = sentences[training_size:]\n",
    "\n",
    "    training_labels = labels[:training_size]\n",
    "    validation_labels = labels[training_size:]\n",
    "\n",
    "    # Fit tokenizer with training data\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type,\n",
    "                                      truncating=trunc_type)\n",
    "\n",
    "    training_label_f = np.array(training_labels)\n",
    "    validation_label_f = np.array(validation_labels)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(training_padded, training_label_f, epochs=15, validation_data=(validation_padded, validation_label_f))\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_C4()\n",
    "    model.save(\"model_C4.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================\n",
    "#\n",
    "# Build and train a neural network model using the Sunspots.csv dataset.\n",
    "# Use MAE as the metrics of your neural network model.\n",
    "# We provided code for normalizing the data. Please do not change the code.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
    "#\n",
    "# Desired MAE < 0.15 on the normalized dataset.\n",
    "# ========================================================================================\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "def solution_A5():\n",
    "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sunspots.csv'\n",
    "    urllib.request.urlretrieve(data_url, 'sunspots.csv')\n",
    "\n",
    "    time_step = []\n",
    "    sunspots = []\n",
    "\n",
    "    with open('sunspots.csv') as csvfile:\n",
    "      reader = csv.reader(csvfile, delimiter=',')\n",
    "      next(reader)\n",
    "      for row in reader:\n",
    "          sunspots.append(float(row[2]))\n",
    "          time_step.append(int(row[0])) \n",
    "\n",
    "    series= np.array(sunspots)\n",
    "    min=np.min(series)\n",
    "    max=np.max(series)\n",
    "    series -= min\n",
    "    series /= max\n",
    "    time=np.array(time_step)\n",
    "\n",
    "    split_time=3000\n",
    "\n",
    "    time_train= time[:split_time] \n",
    "    x_train= series[:split_time] \n",
    "    time_valid= time[split_time:] \n",
    "    x_valid= series[split_time:] \n",
    "\n",
    "    window_size=30\n",
    "    batch_size=32\n",
    "    shuffle_buffer_size=1000\n",
    "\n",
    "\n",
    "    train_set=windowed_dataset(x_train, window_size=window_size,\n",
    "                               batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
    "\n",
    "\n",
    "    model=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(30, input_shape=[None,1], activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(15, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=1))\n",
    "    model.fit(train_set, epochs=500, verbose=0)\n",
    "\n",
    "    forecast=[]\n",
    "    for time in range(len(series) - window_size):\n",
    "      forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "    forecast = forecast[split_time-window_size:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "    print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # DO NOT CHANGE THIS CODE\n",
    "    model=solution_A5()\n",
    "    model.save(\"model_A5.h5\")\n",
    "\n",
    "# ============================================================================================\n",
    "# Build and train a neural network model using the Daily Max Temperature.csv dataset.\n",
    "# Use MAE as the metrics of your neural network model.\n",
    "# We provided code for normalizing the data. Please do not change the code.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
    "#\n",
    "# Desired MAE < 0.2 on the normalized dataset.\n",
    "# ============================================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import urllib\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "def solution_B5():\n",
    "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv'\n",
    "    urllib.request.urlretrieve(data_url, 'daily-max-temperatures.csv')\n",
    "\n",
    "    time_step = []\n",
    "    temps = []\n",
    "\n",
    "    with open('daily-max-temperatures.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        step = 0\n",
    "        for row in reader:\n",
    "            temps.append(float(row[1]))\n",
    "            time_step.append((row[0]))\n",
    "            step=step + 1\n",
    "\n",
    "    series= np.array(temps)\n",
    "\n",
    "    # Normalization Function.\n",
    "    min=np.min(series)\n",
    "    max=np.max(series)\n",
    "    series -= min\n",
    "    series /= max\n",
    "    time=np.array(time_step)\n",
    "\n",
    "    split_time=2500\n",
    "\n",
    "    time_train= time[:split_time]\n",
    "    x_train= series[:split_time]\n",
    "    time_valid= time[split_time:] \n",
    "    x_valid= series[split_time:]\n",
    "\n",
    "    window_size=64\n",
    "    batch_size=256\n",
    "    shuffle_buffer_size=1000\n",
    "\n",
    "    train_set=windowed_dataset(\n",
    "        x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "    print(train_set)\n",
    "    print(x_train.shape)\n",
    "\n",
    "    model=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(30, input_shape=[None,1], activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(15, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=1))\n",
    "    model.fit(train_set, epochs=500, verbose=0)\n",
    "\n",
    "    forecast=[]\n",
    "    for time in range(len(series) - window_size):\n",
    "      forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "    forecast = forecast[split_time-window_size:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "    print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=solution_B5()\n",
    "    model.save(\"model_B5.h5\")\n",
    "\n",
    "# ============================================================================================\n",
    "#\n",
    "# Build and train a neural network model using the Daily Min Temperature.csv dataset.\n",
    "# Use MAE as the metrics of your neural network model.\n",
    "# We provided code for normalizing the data. Please do not change the code.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
    "#\n",
    "# Desired MAE < 0.19 on the normalized dataset.\n",
    "# ============================================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import urllib\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "def solution_C5():\n",
    "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
    "    urllib.request.urlretrieve(data_url, 'daily-min-temperatures.csv')\n",
    "\n",
    "    time_step = []\n",
    "    temps = []\n",
    "\n",
    "    with open('daily-min-temperatures.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        step = 0\n",
    "        for row in reader:\n",
    "            temps.append(float(row[1]))\n",
    "            time_step.append((row[0]))\n",
    "            step=step + 1\n",
    "\n",
    "    series= np.array(temps)\n",
    "\n",
    "    # Normalization Function. \n",
    "    min=np.min(series)\n",
    "    max=np.max(series)\n",
    "    series -= min\n",
    "    series /= max\n",
    "    time=np.array(time_step)\n",
    "\n",
    "    split_time=2500\n",
    "\n",
    "    time_train= time[:split_time]\n",
    "    x_train= series[:split_time]\n",
    "    time_valid= time[split_time:]\n",
    "    x_valid= series[split_time:]\n",
    "\n",
    "    window_size=64\n",
    "    batch_size=256\n",
    "    shuffle_buffer_size=1000\n",
    "\n",
    "    train_set=windowed_dataset(\n",
    "        x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "    print(train_set)\n",
    "    print(x_train.shape)\n",
    "\n",
    "    model=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(30, input_shape=[None,1], activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(15, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=1))\n",
    "    model.fit(train_set, epochs=500, verbose=0)\n",
    "\n",
    "    forecast=[]\n",
    "    for time in range(len(series) - window_size):\n",
    "      forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "    forecast = forecast[split_time-window_size:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "    print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=solution_C5()\n",
    "    model.save(\"model_C5.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
